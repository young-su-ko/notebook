---
layout: post
category: concepts
title: KL divergence
---

The Kullback-Leibler (KL) divergence comes up a lot in ML--like a lot. Most recently, I learned it's used to compute the [Inception Score](/notebook/fid.html). So let's start by going over what it is, the intuition behind it, and why it's so useful[^1].

### what is KL divergence?
If someone were to ask me this, the first response that comes up to my mind is, "it's a way to measure how similar two distributions are." But to be honest, the word **similar** can really mean so many things--how can we be more specific?

KL divergence is defined as:

$$
D_\text{KL}(P \mid \mid Q) = \sum_{x \in \mathcal{X}}P(x)\log \frac{P(x)}{Q(x)}
$$

When I see this, it's not immediately clear what's going on. But I have a few ideas.

<!-- If we set $$f(x) =\log \frac{P(x)}{Q(x)}$$ for brevity, suddenly $$\sum P(x) f(x)$$ looks a lot like an **expectation** for a discrete random variable. So another way we could write our definition is:

$$
D_\text{KL}(P \mid \mid Q) = \mathbb{E}_{x \sim P(x)} \left[\log \frac{P(x)}{Q(x)}\right]
$$

Ok, nice, that means we can conveniently use Monte Carlo estimation later, to calculate a value. But what does this log represent? -->

From log rules, we can re-write $$\log \frac{P(x)}{Q(x)}$$ as $$ \log P(x) - \log Q(x)$$. Is this helpful though? Written out, we get:

$$
= \sum_{x \in \mathcal{X}} P(x)[\log P(x) - \log Q(x)]
$$

$$
= \sum_{x \in \mathcal{X}} P(x)\log P(x) -\sum_{x \in \mathcal{X}}P(x)\log Q(x)
$$


#### entropy + cross entropy
Let's take a detour for a minute to talk about entropy. The Shannon entropy is defined as:

$$
H(X) = - \sum_{i} P(x_i)\log P(x_i)
$$

But how do we intuitively understand this[^2]? In information theory, entropy is often referred to as a measure of surprise. 

Then what does it mean to be **"surprised?"** Intuitively, we are surprised when an unlikely event occurs. Given a probability of an event, $$P(E)$$, surprise should be large if $$P(E)$$ is small, and vice-versa.

Mathematically, surprisal, also known as the information content, is defined as:

$$
I(E) = \log \left( \frac{1}{P(E)} \right)
$$

$$
= -\log P(E)
$$

For example, when $$P(E)$$ is 1, then we get $$I(E) = \log 1 = 0$$. Meaning we are not surprised at all when a guaranteed event occurs.

Now we can understand entropy as the expected surprisal or the average surprise when sampling from a distribution. For a discrete probability distribution, we have:

$$
H(X) = \mathbb{E}[I(E)] = \mathbb{E}[-\log(P(X))]
$$

$$
= -\sum_{x \in \mathcal{X}} P(x)\log P(x)
$$

Given some random variable $$X$$, entropy is a weighted sum: we multiply each outcome's probability, $$P(x)$$, by the surprisal of that outcome, $$-\log P(x)$$. 

Imagine a simple case where we are flipping a coin. Let's compare the entropy for a fair coin and a weighted coin.

|Coin| P(Heads) | P(Tails) | Entropy[^3] |
|--- | --- | --- | --- |
|Fair | 0.5 | 0.5 |  1 bit |
|Weighted | 0.9 | 0.1 | 0.47 bits |

A fair coin has the highest possible entropy: 1 bit. A weighted coin has a lower entropy because in most cases, we will get heads and won't be surprised by this.

Then what is **cross entropy**? It's defined as:

$$
H(P,Q) = \mathbb{E}_{x \sim P}[-\log Q(x)]
$$

$$
= -\sum_{x} P(x)\log Q(x)
$$

We sample from $$P$$ but now measure surprise using $$Q$$. Or in other words, the average surprise given a true distribution $$P$$ but we believe the distribution $$Q$$ to be true.

While I understood the equations, I didn't get the intuition. So let's try to write out a specific example, using the coin example again. Let's say we have a fair coin, but we believe it is actually weighted, at 60/40 heads to tails.

$$
H(P,Q) = -(0.5 \log 0.6 + 0.5 \log 0.4)
$$

$$
= 1.03
$$

What if we believe an even more incorrect distribution, say 90/10?

$$
H(P,Q) = -(0.5 \log 0.9 + 0.5 \log 0.1)
$$

$$
= 1.74
$$

What if we also believe it is a fair coin?

$$
H(P,Q) = -(0.5 \log 0.5 + 0.5 \log 0.5)
$$

$$
= 1 = H(P)
$$

Interesting, it seems that $$H(P,Q) \geq H(P)$$, only equal when $$P=Q$$. Then cross entropy can be thought of as

$$
H(P,Q) = H(P) + C
$$

**Cross entropy is the sum of the inherent surprise of $$P$$, measured by $$H(P)$$, and an error term, $$C$$**. This error arises from the surprise incurred from assuming $$Q$$ but in reality $$P$$ is true. When $$Q=P$$, $$C=0$$ and the error term disappears.

If we solve for this error term, we get:

$$
C = H(P,Q) - H(P)
$$


### connecting the dots
Wait, what? From the beginning of this post, we said KL divergence can be written as:

$$
= \sum_{x \in \mathcal{X}} P(x)\log P(x) -\sum_{x \in \mathcal{X}}P(x)\log Q(x)
$$

After re-arranging, we get that:

$$
D_\text{KL}(P \mid \mid Q) = H(P,Q) - H(P)
$$

So we get that:

$$
D_\text{KL}(P \mid \mid Q) = C
$$

So the KL divergence is just measuring the **extra surprise** for believing $$Q$$ when $$P$$ is true. 

Now we can see why KL divergence is used to compare distributions $$P$$ and $$Q$$. If we believe $$P$$ and $$Q=P$$, the we have no error term, the KL divergence is 0. If $$Q$$ is very different from $$P$$, the error term will be large, leading to a large KL divergence.


---
{: data-content="footnotes"}
[^1]: For reference, I'm going off the [Wikipedia page](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).
[^2]: Also going off the [Wikipedia page](https://en.wikipedia.org/wiki/Entropy_(information_theory)) for entropy.
[^3]: Using log base 2, which gives us the standard unit for entropy, the bit.