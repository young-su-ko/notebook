---
layout: post
category: concepts
title: KL divergence
---

The Kullback-Leibler (KL) divergence comes up a lot in ML--like a lot. Most recently, I learned it's used to compute the [Inception Score](/notebook/fid.html). So let's start by going over what it is, the intuition behind it, and why it's so useful[^1].

### what is KL divergence?
If someone were to ask me this, the first response that comes up to my mind is, "it's a way to measure how similar two distributions are." But to be honest, the word **similar** can really mean so many things--how can we be more specific?

KL divergence is defined as:

$$
D_\text{KL}(P \mid \mid Q) = \sum_{x \in \mathcal{X}}P(x)\log \frac{P(x)}{Q(x)}
$$

When I see this, it's not immediately clear what's going on. But I have a few ideas.

<!-- If we set $$f(x) =\log \frac{P(x)}{Q(x)}$$ for brevity, suddenly $$\sum P(x) f(x)$$ looks a lot like an **expectation** for a discrete random variable. So another way we could write our definition is:

$$
D_\text{KL}(P \mid \mid Q) = \mathbb{E}_{x \sim P(x)} \left[\log \frac{P(x)}{Q(x)}\right]
$$

Ok, nice, that means we can conveniently use Monte Carlo estimation later, to calculate a value. But what does this log represent? -->

From log rules, we can re-write $$\log \frac{P(x)}{Q(x)}$$ as $$ \log P(x) - \log Q(x)$$. Is this helpful though? Written out, we get:

$$
= \sum_{x \in \mathcal{X}} P(x)[\log P(x) - \log Q(x)]
$$

$$
= \sum_{x \in \mathcal{X}} P(x)\log P(x) -\sum_{x \in \mathcal{X}}P(x)\log Q(x)
$$


#### entropy + cross entropy
In information theory, entropy is often referred to as a measure of surprise. Another phrase I often hear is "how much information we gain".



The Shannon Entropy is defined as:

$$
H(X) = - \sum_{i} P(x_i)\log P(x_i)
$$

---
{: data-content="footnotes"}
[^1]: For reference, I'm going off the [Wikipedia page](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).