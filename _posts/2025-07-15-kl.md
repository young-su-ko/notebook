---
layout: post
category: concepts
title: KL divergence
---

The Kullback-Leibler (KL) divergence comes up a lot in ML--like a lot. Most recently, I learned it's used to compute the [Inception Score](/notebook/fid.html). So let's start by going over what it is, the intuition behind it, and why it's so useful[^1].

### what is KL divergence?
If someone were to ask me this, the first response that comes up to my mind is, "it's a way to measure how similar two distributions are." But to be honest, the word **similar** can really mean so many things--how can we be more specific?

KL divergence is defined as:

$$
D_\text{KL}(P \mid \mid Q) = \sum_{x \in \mathcal{X}}P(x)\log \frac{P(x)}{Q(x)}
$$

When I see this, it's not immediately clear what's going on. But I have a few ideas.

If we set $$f(x) =\log \frac{P(x)}{Q(x)}$$ for brevity, suddently $$\sum P(x) f(x)$$ looks a lot like an **expectation** for a discrete random variable. So another way we could write our definition is:

$$
D_\text{KL}(P \mid \mid Q) = \mathbb{E}_{x \sim P(x)} \left[\log \frac{P(x)}{Q(x)}\right]
$$

Ok, nice, that means we can conveniently use Monte Carlo estimation later, to calculate a value. But what does this log represent?

Hmm, from log rules, we can re-write $$\log \frac{P(x)}{Q(x)}$$ as $$ \log P(x) - \log Q(x)$$. Is this helpful though?

---
{: data-content="footnotes"}
[^1]: For reference, I'm going off the [Wikipedia page](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).