---
layout: post
category: concepts
title: fréchet inception distance
---

I recently watched MIT's [Introduction to Flow Matching and Diffusion Models](https://diffusion.csail.mit.edu/) course[^1]. It's made me interested in generative models and their applications in protein engineering. But before diving into generative models for proteins, I wanted to start off with something simpler, and luckily one of the course assignments is training a flow matching model on MNIST.

After training this model, I was super excited and showed my PI my generated digits. I was also showing him how, with classifier-free guidance, you can change the guidance value to get "better" images. He asked me if there's a quantitative way to measure how much "better" the images are, and I didn't have a great answer[^2]. I just assumed you go off of vibes.

But after reading some more image generation papers like Imagen[^3], I realized that Fréchet inception distance (FID)[^4] existed and was used to assess image quality (ah, of course, just  evaluating on vibes was not enough).

---
{: data-content="footnotes"}
[^1]: This was a great series of lectures. The accompanying [lecture notes](https://diffusion.csail.mit.edu/docs/lecture-notes.pdf) are written very clearly. It was also very fun to go back and forth with 4o while referencing the lecture notes.
[^2]: Maybe this was covered in the lecture series, but I couldn't think of an answer on the spot.
[^3]: [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/abs/2205.11487)
[^4]: Also I *just* realized the [dWJS](https://arxiv.org/pdf/2306.12360) paper's distributional conformity score is motivated by FID!!